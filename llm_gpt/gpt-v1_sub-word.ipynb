{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import regex as re\n",
    "\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "file_path = 'C:/Code_Projects/llm/datasets/'\n",
    "train_file_path = file_path + 'sw_train_split.txt'\n",
    "val_file_path = file_path + 'sw_val_split.txt'\n",
    "vocab_file_path = file_path + 'sw_vocab.txt'\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 64\n",
    "learning_rate = 3e-4\n",
    "n_embd = 384 #384\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: distresses \n",
      "impos \n",
      "Inglourious \n",
      "Healy \n",
      "Republicans \n",
      "MLB \n",
      "Taoist \n",
      "sash \n",
      "Flick \n",
      "whirled \n",
      "FitzChivalry \n"
     ]
    }
   ],
   "source": [
    "def Convert(string): \n",
    "    li = list(string.split('\\n')) \n",
    "    return li\n",
    "\n",
    "with open(vocab_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'text: {text[:100]}')\n",
    "sub_word = Convert(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#huggingface BPE tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(ids):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = train_file_path if split == 'train' else val_file_path\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            max_start_pos = max(0, file_size - block_size * batch_size)\n",
    "            start_pos = random.randint(0, max_start_pos)\n",
    "            # file 1000\n",
    "            # block 16\n",
    "            # batch 8\n",
    "            \n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size)\n",
    "\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    \n",
    "    if len(data) < block_size + 1:\n",
    "        # raise ValueError\n",
    "        print(f\"Data length ({len(data)}) is too short for the specified block size ({block_size})\")\n",
    "        data = get_random_chunk(split)\n",
    "\n",
    "    max_index = len(data) - block_size - 1\n",
    "    if max_index <= 0:\n",
    "        raise ValueError(f\"Max index ({max_index}) must be greater than 0. Adjust block size or ensure sufficient data length.\")\n",
    "    \n",
    "    ix = torch.randint(0, max_index + 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 64])\n",
      "targets:\n",
      "torch.Size([32, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\ZeroIchi\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\ZeroIchi\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\ZeroIchi\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ZeroIchi\\AppData\\Local\\Temp\\ipykernel_22648\\1237237131.py\", line 1, in <module>\n",
      "    x, y = get_batch('train')\n",
      "  File \"C:\\Users\\ZeroIchi\\AppData\\Local\\Temp\\ipykernel_22648\\3862663193.py\", line 27, in get_batch\n",
      "    data = get_random_chunk(split)\n",
      "  File \"C:\\Users\\ZeroIchi\\AppData\\Local\\Temp\\ipykernel_22648\\3862663193.py\", line 22, in get_random_chunk\n",
      "    data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
      "C:\\Users\\ZeroIchi\\AppData\\Local\\Temp\\ipykernel_22648\\3862663193.py:22: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(encode(decoded_block), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x.shape)\n",
    "print('targets:')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "# [1, 0, 0]\n",
    "# [1, 0.6, 0]\n",
    "# [1, 0.6, 0.4]\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "    \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        # print(Fore.BLUE + f'B,T: {index}')\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'C:/Code_Projects/llm/trained_models/'\n",
    "model_name = 'SW_V'\n",
    "saved_model =  model_path + 'SW_V_3230.pkl'\n",
    "use_trained_model = True\n",
    "max_iters = 50000\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model parameters...\n",
      "loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "if(use_trained_model):\n",
    "    print('loading model parameters...')\n",
    "    with open(saved_model, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print('loaded successfully!')\n",
    "\n",
    "\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 0, train loss: 6.1205, eval loss: 6.0733##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_0.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 100, train loss: 6.0357, eval loss: 5.9722##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_10.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 200, train loss: 6.0009, eval loss: 6.0808##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_20.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (29) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 300, train loss: 6.0128, eval loss: 6.1614##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_30.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 400, train loss: 6.0668, eval loss: 6.0963##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_40.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (30) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 500, train loss: 5.9879, eval loss: 6.0869##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_50.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 600, train loss: 6.0483, eval loss: 6.0550##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_60.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 700, train loss: 5.9912, eval loss: 5.9760##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_70.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 800, train loss: 6.0362, eval loss: 5.9446##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_80.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 900, train loss: 6.0214, eval loss: 5.9936##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_90.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 1000, train loss: 5.9267, eval loss: 5.9919##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_100.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 1100, train loss: 5.9677, eval loss: 5.9672##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_110.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (30) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 1200, train loss: 6.0314, eval loss: 6.0986##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_120.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 1300, train loss: 5.9519, eval loss: 5.9818##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_130.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 1400, train loss: 5.9526, eval loss: 6.0232##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_140.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 1500, train loss: 5.9923, eval loss: 6.0788##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_150.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 1600, train loss: 6.0920, eval loss: 5.9620##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_160.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (29) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 1700, train loss: 6.0631, eval loss: 5.8740##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_170.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 1800, train loss: 5.9333, eval loss: 5.8579##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_180.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 1900, train loss: 5.9053, eval loss: 6.0019##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_190.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 2000, train loss: 6.0150, eval loss: 6.0163##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_200.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 2100, train loss: 5.9070, eval loss: 5.9871##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_210.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (36) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 2200, train loss: 5.8857, eval loss: 5.8515##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_220.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 2300, train loss: 5.9226, eval loss: 5.9213##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_230.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 2400, train loss: 5.9437, eval loss: 5.8442##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_240.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 2500, train loss: 5.8112, eval loss: 5.9666##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_250.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 2600, train loss: 5.8438, eval loss: 5.9318##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_260.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 2700, train loss: 5.8737, eval loss: 5.9345##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_270.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 2800, train loss: 5.9407, eval loss: 5.9846##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_280.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 2900, train loss: 6.0343, eval loss: 5.9138##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_290.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 3000, train loss: 5.8800, eval loss: 5.9508##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_300.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 3100, train loss: 5.8887, eval loss: 5.8771##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_310.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 3200, train loss: 5.8428, eval loss: 5.8750##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_320.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 3300, train loss: 5.8066, eval loss: 5.8475##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_330.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 3400, train loss: 5.8342, eval loss: 5.8851##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_340.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 3500, train loss: 5.8202, eval loss: 5.9332##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_350.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 3600, train loss: 5.9170, eval loss: 5.7741##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_360.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 3700, train loss: 5.9171, eval loss: 5.7908##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_370.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (22) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 3800, train loss: 5.8325, eval loss: 5.8944##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_380.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 3900, train loss: 5.8986, eval loss: 6.0451##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_390.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 4000, train loss: 5.8407, eval loss: 5.7554##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_400.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 4100, train loss: 5.8297, eval loss: 5.8946##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_410.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 4200, train loss: 5.7345, eval loss: 5.8875##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_420.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 4300, train loss: 5.7842, eval loss: 5.8033##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_430.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (51) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 4400, train loss: 5.9174, eval loss: 5.9899##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_440.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 4500, train loss: 5.7761, eval loss: 5.9038##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_450.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 4600, train loss: 5.7917, eval loss: 5.7924##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_460.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 4700, train loss: 5.7439, eval loss: 5.8028##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_470.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 4800, train loss: 5.8100, eval loss: 5.8547##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_480.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 4900, train loss: 5.8340, eval loss: 5.9297##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_490.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (28) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 5000, train loss: 5.7167, eval loss: 5.8150##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_500.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 5100, train loss: 5.8141, eval loss: 5.7899##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_510.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 5200, train loss: 5.9230, eval loss: 5.8758##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_520.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 5300, train loss: 5.9817, eval loss: 5.8935##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_530.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 5400, train loss: 5.8310, eval loss: 5.8777##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_540.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 5500, train loss: 5.6838, eval loss: 6.0082##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_550.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 5600, train loss: 5.7938, eval loss: 5.7652##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_560.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 5700, train loss: 5.7005, eval loss: 5.8233##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_570.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 5800, train loss: 5.7549, eval loss: 5.7683##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_580.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 5900, train loss: 5.8291, eval loss: 5.5994##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_590.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6000, train loss: 5.7961, eval loss: 5.7373##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_600.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (25) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 6100, train loss: 5.8676, eval loss: 5.7098##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_610.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (30) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 6200, train loss: 5.7729, eval loss: 5.7088##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_620.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6300, train loss: 5.8578, eval loss: 5.7976##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_630.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6400, train loss: 5.8232, eval loss: 5.8759##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_640.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6500, train loss: 5.8519, eval loss: 5.8071##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_650.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6600, train loss: 5.6380, eval loss: 5.7976##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_660.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6700, train loss: 5.7818, eval loss: 5.7246##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_670.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6800, train loss: 5.6678, eval loss: 5.7365##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_680.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 6900, train loss: 5.7464, eval loss: 5.6985##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_690.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7000, train loss: 5.6165, eval loss: 5.7992##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_700.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 7100, train loss: 5.6669, eval loss: 5.7478##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_710.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7200, train loss: 5.7923, eval loss: 5.7844##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_720.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 7300, train loss: 5.8105, eval loss: 5.6664##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_730.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7400, train loss: 5.7283, eval loss: 5.6959##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_740.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (50) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7500, train loss: 5.7733, eval loss: 5.7457##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_750.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7600, train loss: 5.7150, eval loss: 5.7476##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_760.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7700, train loss: 5.7953, eval loss: 5.7333##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_770.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 7800, train loss: 5.8006, eval loss: 5.5962##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_780.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (34) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 7900, train loss: 5.7092, eval loss: 5.7232##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_790.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 8000, train loss: 5.6696, eval loss: 5.7472##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_800.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 8100, train loss: 5.6985, eval loss: 5.8811##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_810.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 8200, train loss: 5.6687, eval loss: 5.6134##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_820.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 8300, train loss: 5.7492, eval loss: 5.6328##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_830.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 8400, train loss: 5.5410, eval loss: 5.7513##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_840.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 8500, train loss: 5.8376, eval loss: 5.8317##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_850.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 8600, train loss: 5.6423, eval loss: 5.7609##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_860.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 8700, train loss: 5.6338, eval loss: 5.7049##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_870.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (34) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 8800, train loss: 5.7474, eval loss: 5.6408##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_880.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 8900, train loss: 5.6316, eval loss: 5.7246##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_890.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (39) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9000, train loss: 5.7179, eval loss: 5.7256##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_900.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 9100, train loss: 5.7163, eval loss: 5.6904##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_910.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9200, train loss: 5.5683, eval loss: 5.6719##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_920.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9300, train loss: 5.7261, eval loss: 5.6690##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_930.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9400, train loss: 5.6459, eval loss: 5.6860##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_940.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9500, train loss: 5.6963, eval loss: 5.8254##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_950.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9600, train loss: 5.6110, eval loss: 5.7719##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_960.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 9700, train loss: 5.7383, eval loss: 5.6679##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_970.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9800, train loss: 5.6820, eval loss: 5.7562##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_980.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 9900, train loss: 5.6621, eval loss: 5.7281##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_990.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10000, train loss: 5.6714, eval loss: 5.7018##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1000.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 10100, train loss: 5.7893, eval loss: 5.6337##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1010.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10200, train loss: 5.6919, eval loss: 5.5858##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1020.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10300, train loss: 5.5406, eval loss: 5.5698##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1030.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10400, train loss: 5.6007, eval loss: 5.6613##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1040.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 10500, train loss: 5.6170, eval loss: 5.6624##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1050.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10600, train loss: 5.6387, eval loss: 5.7575##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1060.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 10700, train loss: 5.9548, eval loss: 5.7814##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1070.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10800, train loss: 5.6280, eval loss: 5.6098##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1080.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 10900, train loss: 5.5859, eval loss: 5.6799##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1090.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 11000, train loss: 5.5720, eval loss: 5.7363##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1100.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 11100, train loss: 5.6443, eval loss: 5.7110##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1110.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (31) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 11200, train loss: 5.6066, eval loss: 5.5936##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1120.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 11300, train loss: 5.7529, eval loss: 5.6132##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1130.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 11400, train loss: 5.5955, eval loss: 5.6847##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1140.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 11500, train loss: 5.5546, eval loss: 5.6022##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1150.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 11600, train loss: 5.6039, eval loss: 5.6271##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1160.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 11700, train loss: 5.6983, eval loss: 5.5861##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1170.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 11800, train loss: 5.6512, eval loss: 5.7008##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1180.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 11900, train loss: 5.5889, eval loss: 5.6127##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1190.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 12000, train loss: 5.6438, eval loss: 5.6829##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1200.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12100, train loss: 5.5409, eval loss: 5.6925##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1210.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 12200, train loss: 5.6383, eval loss: 5.7466##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1220.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12300, train loss: 5.5754, eval loss: 5.6534##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1230.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12400, train loss: 5.6402, eval loss: 5.6260##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1240.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12500, train loss: 5.6613, eval loss: 5.5828##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1250.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12600, train loss: 5.7224, eval loss: 5.7440##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1260.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 12700, train loss: 5.6173, eval loss: 5.7376##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1270.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (18) is too short for the specified block size (64)\n",
      "Data length (17) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12800, train loss: 5.6852, eval loss: 5.6174##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1280.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 12900, train loss: 5.5482, eval loss: 5.6147##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1290.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (23) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 13000, train loss: 5.5517, eval loss: 5.6011##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1300.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 13100, train loss: 5.6998, eval loss: 5.6103##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1310.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 13200, train loss: 5.5700, eval loss: 5.6160##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1320.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 13300, train loss: 5.6468, eval loss: 5.6036##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1330.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 13400, train loss: 5.5412, eval loss: 5.5267##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1340.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 13500, train loss: 5.6109, eval loss: 5.6412##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1350.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (10) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 13600, train loss: 5.5429, eval loss: 5.5374##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1360.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (16) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 13700, train loss: 5.6461, eval loss: 5.6428##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1370.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 13800, train loss: 5.5953, eval loss: 5.6209##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1380.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (32) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 13900, train loss: 5.6671, eval loss: 5.6062##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1390.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (32) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 14000, train loss: 5.5457, eval loss: 5.6653##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1400.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 14100, train loss: 5.6465, eval loss: 5.6809##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1410.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 14200, train loss: 5.5760, eval loss: 5.5690##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1420.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 14300, train loss: 5.5412, eval loss: 5.5677##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1430.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 14400, train loss: 5.6671, eval loss: 5.6340##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1440.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 14500, train loss: 5.5759, eval loss: 5.6843##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1450.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 14600, train loss: 5.5651, eval loss: 5.5886##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1460.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 14700, train loss: 5.5666, eval loss: 5.5323##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1470.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 14800, train loss: 5.5938, eval loss: 5.5482##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1480.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 14900, train loss: 5.5352, eval loss: 5.5253##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1490.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 15000, train loss: 5.6706, eval loss: 5.5333##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1500.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 15100, train loss: 5.5343, eval loss: 5.5466##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1510.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 15200, train loss: 5.5767, eval loss: 5.5989##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1520.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 15300, train loss: 5.5243, eval loss: 5.5918##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1530.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 15400, train loss: 5.6456, eval loss: 5.6351##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1540.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 15500, train loss: 5.7343, eval loss: 5.5835##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1550.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 15600, train loss: 5.5584, eval loss: 5.5323##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1560.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 15700, train loss: 5.6320, eval loss: 5.5005##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1570.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 15800, train loss: 5.5591, eval loss: 5.4826##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1580.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 15900, train loss: 5.6868, eval loss: 5.6489##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1590.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (8) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 16000, train loss: 5.5501, eval loss: 5.5521##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1600.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 16100, train loss: 5.5565, eval loss: 5.5751##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1610.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 16200, train loss: 5.5394, eval loss: 5.6152##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1620.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 16300, train loss: 5.5398, eval loss: 5.6200##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1630.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 16400, train loss: 5.4742, eval loss: 5.6177##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1640.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 16500, train loss: 5.6777, eval loss: 5.5255##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1650.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 16600, train loss: 5.4870, eval loss: 5.4560##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1660.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 16700, train loss: 5.5892, eval loss: 5.4708##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1670.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 16800, train loss: 5.6628, eval loss: 5.6121##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1680.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 16900, train loss: 5.5569, eval loss: 5.5029##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1690.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (33) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 17000, train loss: 5.5623, eval loss: 5.5732##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1700.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 17100, train loss: 5.5280, eval loss: 5.5567##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1710.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 17200, train loss: 5.5416, eval loss: 5.4818##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1720.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 17300, train loss: 5.5499, eval loss: 5.5100##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1730.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 17400, train loss: 5.4845, eval loss: 5.6262##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1740.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 17500, train loss: 5.5108, eval loss: 5.5297##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1750.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 17600, train loss: 5.5076, eval loss: 5.5742##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1760.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 17700, train loss: 5.6026, eval loss: 5.5896##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1770.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 17800, train loss: 5.5902, eval loss: 5.4468##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1780.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 17900, train loss: 5.5008, eval loss: 5.5420##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1790.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 18000, train loss: 5.4466, eval loss: 5.5888##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1800.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 18100, train loss: 5.5627, eval loss: 5.5374##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1810.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 18200, train loss: 5.5265, eval loss: 5.4302##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1820.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 18300, train loss: 5.6591, eval loss: 5.5005##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1830.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 18400, train loss: 5.5603, eval loss: 5.5699##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1840.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 18500, train loss: 5.5648, eval loss: 5.4818##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1850.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 18600, train loss: 5.5440, eval loss: 5.5244##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1860.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 18700, train loss: 5.5477, eval loss: 5.4911##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1870.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (24) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 18800, train loss: 5.4663, eval loss: 5.4127##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1880.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 18900, train loss: 5.4312, eval loss: 5.5844##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1890.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 19000, train loss: 5.4742, eval loss: 5.5546##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1900.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 19100, train loss: 5.6082, eval loss: 5.5341##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1910.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 19200, train loss: 5.5377, eval loss: 5.5721##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1920.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 19300, train loss: 5.6155, eval loss: 5.6409##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1930.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 19400, train loss: 5.5086, eval loss: 5.4210##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1940.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (42) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 19500, train loss: 5.5438, eval loss: 5.5848##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1950.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (59) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 19600, train loss: 5.5835, eval loss: 5.5631##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1960.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 19700, train loss: 5.4298, eval loss: 5.6576##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1970.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 19800, train loss: 5.5961, eval loss: 5.4465##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1980.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 19900, train loss: 5.5031, eval loss: 5.4701##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_1990.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (46) is too short for the specified block size (64)\n",
      "Data length (6) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 20000, train loss: 5.4571, eval loss: 5.4835##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2000.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (26) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 20100, train loss: 5.4426, eval loss: 5.4644##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2010.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 20200, train loss: 5.5514, eval loss: 5.5427##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2020.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 20300, train loss: 5.5137, eval loss: 5.5224##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2030.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (58) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 20400, train loss: 5.4806, eval loss: 5.5734##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2040.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 20500, train loss: 5.4339, eval loss: 5.5975##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2050.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 20600, train loss: 5.5185, eval loss: 5.4943##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2060.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 20700, train loss: 5.5264, eval loss: 5.5602##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2070.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (22) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 20800, train loss: 5.5319, eval loss: 5.4062##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2080.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 20900, train loss: 5.5305, eval loss: 5.5455##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2090.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 21000, train loss: 5.4849, eval loss: 5.5243##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2100.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 21100, train loss: 5.4925, eval loss: 5.5123##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2110.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (57) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 21200, train loss: 5.5441, eval loss: 5.4279##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2120.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 21300, train loss: 5.4800, eval loss: 5.5092##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2130.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 21400, train loss: 5.4732, eval loss: 5.3880##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2140.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 21500, train loss: 5.4751, eval loss: 5.5047##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2150.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 21600, train loss: 5.4940, eval loss: 5.3873##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2160.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 21700, train loss: 5.5410, eval loss: 5.4317##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2170.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 21800, train loss: 5.4943, eval loss: 5.5254##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2180.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 21900, train loss: 5.5421, eval loss: 5.3866##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2190.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 22000, train loss: 5.4314, eval loss: 5.4760##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2200.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 22100, train loss: 5.4798, eval loss: 5.3433##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2210.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 22200, train loss: 5.4186, eval loss: 5.4694##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2220.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 22300, train loss: 5.4999, eval loss: 5.4460##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2230.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 22400, train loss: 5.5005, eval loss: 5.3928##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2240.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 22500, train loss: 5.5349, eval loss: 5.5843##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2250.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (59) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 22600, train loss: 5.3999, eval loss: 5.4157##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2260.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 22700, train loss: 5.5265, eval loss: 5.4582##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2270.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 22800, train loss: 5.4158, eval loss: 5.4921##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2280.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 22900, train loss: 5.5408, eval loss: 5.4253##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2290.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 23000, train loss: 5.3879, eval loss: 5.4541##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2300.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 23100, train loss: 5.4831, eval loss: 5.3423##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2310.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 23200, train loss: 5.4624, eval loss: 5.4300##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2320.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 23300, train loss: 5.5540, eval loss: 5.5036##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2330.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 23400, train loss: 5.5505, eval loss: 5.4513##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2340.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 23500, train loss: 5.4025, eval loss: 5.4715##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2350.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 23600, train loss: 5.5103, eval loss: 5.5062##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2360.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 23700, train loss: 5.4758, eval loss: 5.4383##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2370.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 23800, train loss: 5.3947, eval loss: 5.4502##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2380.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 23900, train loss: 5.4498, eval loss: 5.3974##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2390.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24000, train loss: 5.4079, eval loss: 5.5448##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2400.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 24100, train loss: 5.4742, eval loss: 5.4877##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2410.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24200, train loss: 5.4025, eval loss: 5.4868##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2420.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 24300, train loss: 5.5453, eval loss: 5.4258##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2430.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24400, train loss: 5.5201, eval loss: 5.3938##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2440.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 24500, train loss: 5.4211, eval loss: 5.5297##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2450.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24600, train loss: 5.4724, eval loss: 5.3384##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2460.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (39) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24700, train loss: 5.3767, eval loss: 5.4099##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2470.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24800, train loss: 5.5564, eval loss: 5.4430##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2480.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (22) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 24900, train loss: 5.3653, eval loss: 5.6195##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2490.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 25000, train loss: 5.5300, eval loss: 5.6618##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2500.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 25100, train loss: 5.3909, eval loss: 5.4405##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2510.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 25200, train loss: 5.3351, eval loss: 5.4949##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2520.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 25300, train loss: 5.4410, eval loss: 5.5446##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2530.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 25400, train loss: 5.4904, eval loss: 5.4598##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2540.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 25500, train loss: 5.5441, eval loss: 5.3605##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2550.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 25600, train loss: 5.4213, eval loss: 5.4020##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2560.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 25700, train loss: 5.3709, eval loss: 5.4695##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2570.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 25800, train loss: 5.5120, eval loss: 5.4339##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2580.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 25900, train loss: 5.4416, eval loss: 5.4844##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2590.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26000, train loss: 5.4590, eval loss: 5.4762##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2600.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26100, train loss: 5.3895, eval loss: 5.5799##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2610.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (31) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26200, train loss: 5.4465, eval loss: 5.3955##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2620.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (33) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26300, train loss: 5.4552, eval loss: 5.5159##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2630.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26400, train loss: 5.3856, eval loss: 5.4967##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2640.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26500, train loss: 5.5162, eval loss: 5.5315##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2650.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26600, train loss: 5.4829, eval loss: 5.4307##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2660.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 26700, train loss: 5.4951, eval loss: 5.4476##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2670.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 26800, train loss: 5.3356, eval loss: 5.3688##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2680.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 26900, train loss: 5.3936, eval loss: 5.3982##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2690.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 27000, train loss: 5.3599, eval loss: 5.4376##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2700.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 27100, train loss: 5.4506, eval loss: 5.4343##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2710.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 27200, train loss: 5.4473, eval loss: 5.4516##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2720.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 27300, train loss: 5.5228, eval loss: 5.3503##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2730.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 27400, train loss: 5.3452, eval loss: 5.4782##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2740.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 27500, train loss: 5.4160, eval loss: 5.4697##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2750.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 27600, train loss: 5.5664, eval loss: 5.5662##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2760.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (34) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 27700, train loss: 5.4785, eval loss: 5.3606##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2770.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 27800, train loss: 5.4384, eval loss: 5.3365##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2780.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 27900, train loss: 5.4131, eval loss: 5.4238##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2790.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28000, train loss: 5.4512, eval loss: 5.3628##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2800.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (57) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28100, train loss: 5.4010, eval loss: 5.3368##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2810.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (35) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28200, train loss: 5.4185, eval loss: 5.4264##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2820.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (64) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28300, train loss: 5.4607, eval loss: 5.4408##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2830.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28400, train loss: 5.4385, eval loss: 5.3839##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2840.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 28500, train loss: 5.4404, eval loss: 5.4327##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2850.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 28600, train loss: 5.4728, eval loss: 5.4852##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2860.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28700, train loss: 5.3463, eval loss: 5.3285##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2870.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28800, train loss: 5.3623, eval loss: 5.4727##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2880.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (34) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 28900, train loss: 5.3930, eval loss: 5.3811##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2890.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 29000, train loss: 5.4867, eval loss: 5.3899##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2900.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 29100, train loss: 5.4083, eval loss: 5.5418##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2910.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 29200, train loss: 5.4835, eval loss: 5.3916##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2920.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (64) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 29300, train loss: 5.4083, eval loss: 5.4322##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2930.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 29400, train loss: 5.3345, eval loss: 5.5134##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2940.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 29500, train loss: 5.4073, eval loss: 5.4063##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2950.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 29600, train loss: 5.3222, eval loss: 5.5136##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2960.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 29700, train loss: 5.4614, eval loss: 5.4535##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2970.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 29800, train loss: 5.3656, eval loss: 5.4297##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2980.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 29900, train loss: 5.4295, eval loss: 5.3307##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_2990.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (22) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30000, train loss: 5.4001, eval loss: 5.5165##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3000.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30100, train loss: 5.4280, eval loss: 5.4686##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3010.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30200, train loss: 5.3167, eval loss: 5.4186##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3020.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (32) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30300, train loss: 5.4101, eval loss: 5.4024##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3030.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30400, train loss: 5.3616, eval loss: 5.3658##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3040.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30500, train loss: 5.4540, eval loss: 5.2882##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3050.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 30600, train loss: 5.4672, eval loss: 5.4620##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3060.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30700, train loss: 5.3069, eval loss: 5.3109##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3070.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 30800, train loss: 5.4373, eval loss: 5.4231##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3080.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 30900, train loss: 5.3982, eval loss: 5.4812##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3090.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31000, train loss: 5.3989, eval loss: 5.3950##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3100.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31100, train loss: 5.3937, eval loss: 5.3127##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3110.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31200, train loss: 5.3325, eval loss: 5.4288##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3120.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31300, train loss: 5.4335, eval loss: 5.3638##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3130.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31400, train loss: 5.3266, eval loss: 5.4295##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3140.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31500, train loss: 5.4164, eval loss: 5.2927##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3150.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31600, train loss: 5.4087, eval loss: 5.3633##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3160.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (44) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 31700, train loss: 5.2227, eval loss: 5.4679##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3170.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 31800, train loss: 5.4672, eval loss: 5.3765##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3180.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (30) is too short for the specified block size (64)\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 31900, train loss: 5.4232, eval loss: 5.3975##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3190.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 32000, train loss: 5.4050, eval loss: 5.3880##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3200.pkl Saved\n",
      "\u001b[0m\n",
      "\u001b[36m##step: 32100, train loss: 5.3969, eval loss: 5.4472##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3210.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 32200, train loss: 5.4250, eval loss: 5.4795##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3220.pkl Saved\n",
      "\u001b[0m\n",
      "Data length (2) is too short for the specified block size (64)\n",
      "\u001b[36m##step: 32300, train loss: 5.4044, eval loss: 5.3780##\n",
      "C:/Code_Projects/llm/trained_models/SW_V_3230.pkl Saved\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Code_Projects\\llm\\cuda\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(Fore.CYAN + f'##step: {iter}, train loss: {losses[\"train\"]:.4f}, eval loss: {losses[\"val\"]:.4f}##')\n",
    "        \n",
    "        #checkpoint\n",
    "        mn = model_path + model_name + f'_{int(iter/10)}' + '.pkl'\n",
    "        with open(mn, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f'{mn} Saved')\n",
    "        \n",
    "        print(Style.RESET_ALL)\n",
    "        \n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "\n",
    "mn = model_name + f'_Final' + '.pkl'\n",
    "with open(model_path + mn, \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f'{mn} Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion:\n",
      "this is a corpus designed to encode his demise and the Ministry of Economics near the Supreme Church in central Pour da ia Nation . At the Council border , the French colon isation of the Syrian candidate , expressed a point migration deficit of 19 - foot - cave homes and 4 - 1 Weiss of Texas  s universities class than he served in a war era hurt Muslims who saw off of the wall - the air in Fort ress and walk out . Walmart , 35 , PP sentencing , 62 , was child contact at $ 78 , 800 , rone sites . But the source for both parties also has a pipeline div vy settlement centre in 2000 , with away - too long , a little slower about Georgia explained out their team minister than the party . My wife is an exit move toward an end\n"
     ]
    }
   ],
   "source": [
    "# prompt = input(\"Prompt:\\n\")\n",
    "prompt = 'this is a corpus'\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=150)[0].tolist())\n",
    "print(f'Completion:\\n{generated_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you want . road output ? but almost such a reality , Gor man . But she continues regularly from sic , ever , to walk . An anonymous news with her friend , enforcement observers have also raised 22 , 000 miss a Sheriff  s Department of Defence . And because Trump was feeling strongly with her own place . telling Trump given up about bias all mid - excesses  if a man should revoke ched the attack around the border . The army was heavily gestures that were asked to do the winds who were painted and attacked by her pro - Reagan homemade jeans on his hand .  Feinstein Adi is scared of your receivers . When you look at this story , all domestic beings Seab ye notes , there are those that cannot day walk , Christ said .  ce8 .\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=150)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
